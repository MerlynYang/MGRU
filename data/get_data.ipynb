{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import baostock as bs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to calculate the realized volatility of the constituent stocks of the HS300 index. The process involves several steps, starting with fetching the constituent stocks using the Baostock API.\n",
    "\n",
    "We would like to thank [Baostock](http://baostock.com/baostock/index.php/%E9%A6%96%E9%A1%B5) for providing the data used in this research. And to facilitate the readers, we have organized the code into functions. You can simply call these functions to execute the respective tasks.\n",
    "\n",
    "### Get constituen stocks of HS300 index\n",
    "We obtain the constituent stocks of the HS300 index by using the Baostock API at the date of acquisition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_constituen_stocks():\n",
    "    lg = bs.login()\n",
    "\n",
    "    rs = bs.query_hs300_stocks(date='2024-1-6')\n",
    "\n",
    "    # get hs300 constituent stocks\n",
    "    hs300_stocks = []\n",
    "    while (rs.error_code == '0') & rs.next():\n",
    "        hs300_stocks.append(rs.get_row_data())\n",
    "    hs300 = pd.DataFrame(hs300_stocks, columns=rs.fields)\n",
    "    \n",
    "    bs.logout()\n",
    "    \n",
    "    hs300.to_csv('hs300_stocks.csv', index=True)\n",
    "    return hs300\n",
    "\n",
    "# get_constituen_stocks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Stock Data Information\n",
    "\n",
    "After obtaining the constituent stocks of the HS300 index, we complete the dataset by adding additional information such as:\n",
    "\n",
    "- Abbreviated Symbol\n",
    "- Sector Code\n",
    "- Sector Code Name\n",
    "- Industry Code\n",
    "- Industry Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_order_id(code):\n",
    "    if code[:3] == 'sh.':\n",
    "        return code[3:] + '.XSHG'\n",
    "    elif code[:3] == 'sz.':\n",
    "        return code[3:] + '.XSHE'\n",
    "    else:\n",
    "        Warning('code is not valid')\n",
    "        return None\n",
    "\n",
    "# add abbrev_symbol, sector_code, sector_code_name, industry_code, industry_name\n",
    "def add_extra_info():\n",
    "    hs300_stocks = pd.read_csv('hs300_stocks.csv', index_col=0)\n",
    "    all_stocks = pd.read_csv('overall_description.csv')\n",
    "\n",
    "    for code in hs300_stocks['code']:\n",
    "        order_id = code_order_id(code)\n",
    "        stock_info = all_stocks[all_stocks['order_book_id'] == order_id]\n",
    "        if len(stock_info) == 0:\n",
    "            print(f'stock info not found for {code}/{order_id}')\n",
    "            continue\n",
    "        hs300_stocks.loc[hs300_stocks['code'] == code, 'abbrev_symbol'] = stock_info['abbrev_symbol'].item()\n",
    "        hs300_stocks.loc[hs300_stocks['code'] == code, 'sector_code'] = stock_info['sector_code'].item()\n",
    "        hs300_stocks.loc[hs300_stocks['code'] == code, 'sector_code_name'] = stock_info['sector_code_name'].item()\n",
    "        hs300_stocks.loc[hs300_stocks['code'] == code, 'industry_code'] = stock_info['industry_code'].item()\n",
    "        hs300_stocks.loc[hs300_stocks['code'] == code, 'industry_name'] = stock_info['industry_name'].item()\n",
    "        \n",
    "    hs300_stocks.to_csv('hs300_stocks.csv', index=True)\n",
    "\n",
    "def group_by_industry():\n",
    "    hs300_stocks = pd.read_csv('hs300_stocks.csv', index_col=0)\n",
    "    grouped = hs300_stocks.sort_values('sector_code').reset_index()\n",
    "    grouped = grouped.rename(columns={'index' : 'original_index'})\n",
    "    grouped.to_csv('hs300_stocks.csv', index=True)\n",
    "\n",
    "# add_extra_info()\n",
    "# group_by_industry()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquire High-Frequency Data\n",
    "\n",
    "To analyze the high-frequency trading patterns of each stock in the HS300 index, we first acquire 5-minute interval data. This data is saved in the `/5mins_data/code.csv` format for each stock.\n",
    "\n",
    "1. **Fetching Data**: Use the Baostock API to fetch the 5-minute interval data.\n",
    "2. **Saving Data**: Save the data in the specified directory.\n",
    "3. **Data Cleaning and Processing**: Perform necessary data cleaning and processing to ensure the data is ready for analysis.\n",
    "\n",
    "*Note: To obtain daily historical data instead of 5-minute intervals, simply change the `frequency` parameter to `'d'` in the `bs.query_history_k_data_plus` function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(code):\n",
    "    start_time = time.time()\n",
    "    rs = bs.query_history_k_data_plus(code,\n",
    "        \"date, time, code, open, high, low, close, volume\",\n",
    "        start_date='2006-01-01', end_date='2023-12-31',\n",
    "        frequency=\"5\", adjustflag=\"1\")\n",
    "    print(f'recived data for {code}, time: {(time.time() - start_time):.1f}')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    data_list = []\n",
    "    while (rs.error_code == '0') & rs.next():\n",
    "        data_list.append(rs.get_row_data())\n",
    "    result = pd.DataFrame(data_list, columns=rs.fields)\n",
    "    result.to_csv(f'./5mins_data/{code}.csv', index=False)\n",
    "    print(f'convert data to dataframe, time: {(time.time() - start_time):.1f}')\n",
    "    \n",
    "    return\n",
    "\n",
    "def get_data():\n",
    "    hs300 = pd.read_csv('hs300_stocks.csv', index_col=0)\n",
    "    bs.login()\n",
    "\n",
    "    code_list = hs300['code']\n",
    "    for i, code in enumerate(code_list):\n",
    "        print(f'get data for {code} ({i + 1}/300)')\n",
    "        if os.path.exists(f'./5mins_data/{code}.csv'):\n",
    "            print(f'{code} already exists, skip')\n",
    "            continue\n",
    "        get_stock_data(code)\n",
    "\n",
    "    bs.logout()\n",
    "\n",
    "# get_data()\n",
    "\n",
    "# change the time format from 20190101093000000000 to 09:30:00\n",
    "def reformulate_time(code):\n",
    "    stock_data = pd.read_csv(f'./5mins_data/{code}.csv')\n",
    "    stock_data['time'] = pd.to_datetime(stock_data['time'], format='%Y%m%d%H%M%S%f')\n",
    "    stock_data['time'] = stock_data['time'].dt.strftime('%H:%M')\n",
    "    stock_data.to_csv(f'./5mins_data/{code}.csv', index=False)\n",
    "\n",
    "# add start_date and end_date of trading to hs300_stocks.csv\n",
    "# reformulate the time format of 5 mins data\n",
    "def add_date_info():\n",
    "    hs300 = pd.read_csv('hs300_stocks.csv', index_col=0)\n",
    "    for code in hs300['code']:\n",
    "        reformulate_time(code)\n",
    "        stock_data = pd.read_csv(f'./5mins_data/{code}.csv')\n",
    "        start_date = stock_data['date'].iloc[0]\n",
    "        end_date = stock_data['date'].iloc[-1]\n",
    "        hs300.loc[hs300['code'] == code, 'start_date'] = start_date\n",
    "        hs300.loc[hs300['code'] == code, 'end_date'] = end_date\n",
    "    hs300.to_csv('hs300_stocks.csv')\n",
    "\n",
    "# add_date_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RV Calculation\n",
    "We calculate the realized volatility for each stock, and then combine it into a single dataframe (/realized_volatility/rv_table.csv) with index to be unique date and columns to be stock code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the realized volatility by intra-day data\n",
    "def cal_rv_daily(intraday_close):\n",
    "    if len(intraday_close) == 0:\n",
    "        return 0\n",
    "    if len(intraday_close) != 44:\n",
    "        return 0\n",
    "    log_intra_price = np.log(intraday_close)\n",
    "    log_intra_return = np.diff(log_intra_price) * 100 # percentage\n",
    "    rv = np.sum(log_intra_return ** 2)\n",
    "    return rv\n",
    "\n",
    "# for each stock, calculate the realized volatility of each day\n",
    "def cal_rv_stock(code):\n",
    "    stock_data = pd.read_csv(f'./5mins_data/{code}.csv')\n",
    "    date_list = stock_data['date'].unique()\n",
    "    rv_list = []\n",
    "    for date in date_list:\n",
    "        filter1 = stock_data['date'] == date\n",
    "        # 09:55 is used to calculate the rv of 10:00\n",
    "        filter2 = stock_data['time'] >= '09:55'\n",
    "        intraday_close = stock_data[filter1 & filter2]['close']\n",
    "        if len(intraday_close) != 43:\n",
    "            Warning(f'code: {code}, date: {date}, there may exist missing data')\n",
    "        rv = cal_rv_daily(intraday_close)\n",
    "        rv_list.append(rv)\n",
    "    \n",
    "    return date_list, rv_list\n",
    "\n",
    "# quick method to calculate the realized volatility of each day\n",
    "def cal_rv_stock_quick(code):\n",
    "    stock_data = pd.read_csv(f'./5mins_data/{code}.csv')\n",
    "    stock_data['log_return'] = np.log(stock_data['close']) - np.log(stock_data['close'].shift(1))\n",
    "    date_list = stock_data['date'].unique()\n",
    "    rv_list = []\n",
    "    filter_time = stock_data[stock_data['time'] >= '10:00']\n",
    "    for date in date_list:\n",
    "        filter1 = filter_time['date'] == date\n",
    "        log_returns = filter_time[filter1]['log_return'] * 100 # percentage\n",
    "        if len(log_returns) != 43:\n",
    "            Warning(f'code: {code}, date: {date}, there may exist missing data')\n",
    "        rv = np.sum(log_returns ** 2)\n",
    "        rv_list.append(rv)\n",
    "    \n",
    "    return date_list, rv_list\n",
    "\n",
    "def cal_rv_all():\n",
    "    hs300 = pd.read_csv('hs300_stocks.csv', index_col=0)\n",
    "    rv_columns = []\n",
    "    \n",
    "    for i, code in enumerate(hs300['code']):\n",
    "        start_time = time.time()\n",
    "        date_list, rv_list = cal_rv_stock_quick(code)\n",
    "        rv_column = pd.DataFrame(rv_list, index=date_list, columns=[code])\n",
    "        rv_columns.append(rv_column)\n",
    "        print(f'{code} ({i + 1}/300) calculated, time used: {(time.time() - start_time):.1f}')\n",
    "\n",
    "    rv_table = pd.concat(rv_columns, axis=1)\n",
    "    rv_table = rv_table.sort_index()\n",
    "    rv_table.to_csv('realized_volatility/rv_table.csv')\n",
    "\n",
    "# cal_rv_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Check the Data\n",
    "\n",
    "It's essential to verify the integrity of the data in the `rv_table`. Specifically, we need to ensure that any `NaN` values in the `rv_table` are due to non-trading days. This step helps in maintaining the accuracy of our analysis and ensures that the missing data points are not due to errors in data acquisition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the trading status stocks of each day\n",
    "def get_tradestatus(code, start_date, end_date):\n",
    "    bs.login()\n",
    "\n",
    "    rs = bs.query_history_k_data_plus(code,\n",
    "        \"date, code, tradestatus, open, high, low, close, volume\",\n",
    "        start_date=start_date, end_date=end_date,\n",
    "        frequency=\"d\", adjustflag=\"1\")\n",
    "\n",
    "    data_list = []\n",
    "    while (rs.error_code == '0') & rs.next():\n",
    "        data_list.append(rs.get_row_data())\n",
    "    result = pd.DataFrame(data_list, columns=rs.fields)\n",
    "    result.to_csv(f'./daily_data/{code}.csv', index=False)\n",
    "    bs.logout()\n",
    "\n",
    "    return result\n",
    "\n",
    "# check the consistency of the trading status and the realized volatility\n",
    "def double_check(code, na_dates_in_rv_table):\n",
    "    not_consistent = []\n",
    "    daily_data = pd.read_csv(f'./daily_data/{code}.csv', index_col=0)\n",
    "    non_trading_dates = daily_data[daily_data['tradestatus'] == 0].index\n",
    "    na_dates_in_rv_table = na_dates_in_rv_table[(na_dates_in_rv_table >= daily_data.index[0]) & (na_dates_in_rv_table <= daily_data.index[-1])]\n",
    "    \n",
    "    na_but_trading = [date for date in na_dates_in_rv_table if date not in non_trading_dates]\n",
    "    na_but_trading_copy = na_but_trading.copy()\n",
    "    non_trading_but_not_na = [date for date in non_trading_dates if date not in na_dates_in_rv_table]\n",
    "    for date in na_but_trading_copy:\n",
    "        if date not in daily_data.index:\n",
    "            na_but_trading.remove(date)\n",
    "    \n",
    "    if len(na_but_trading) != 0:\n",
    "        print(f'code: {code}, na but trading: {na_but_trading}')\n",
    "    if len(non_trading_but_not_na) != 0:\n",
    "        print(f'code: {code}, non trading but not na: {non_trading_but_not_na}')\n",
    "    \n",
    "    not_consistent = na_but_trading + non_trading_but_not_na\n",
    "    \n",
    "    return not_consistent\n",
    "\n",
    "def check_all_date():\n",
    "    rv_table = pd.read_csv('realized_volatility/rv_table.csv', index_col=0)\n",
    "    hs300_stocks = pd.read_csv('hs300_stocks.csv', index_col=0)\n",
    "    code_list = rv_table.columns\n",
    "\n",
    "    for code in code_list:\n",
    "        start_date = hs300_stocks[hs300_stocks['code'] == code]['start_date'].iloc[0]\n",
    "        end_date = hs300_stocks[hs300_stocks['code'] == code]['end_date'].iloc[0]\n",
    "        na_dates = rv_table[rv_table[code].isna()].index\n",
    "        na_dates = na_dates[(na_dates >= start_date) & (na_dates <= end_date)]\n",
    "        # print(na_dates)\n",
    "        get_tradestatus(code, start_date, end_date)\n",
    "        \n",
    "        # not_consistent = double_check(code, na_dates)\n",
    "        # if len(not_consistent) != 0:\n",
    "        #     print(f'double check of code: {code} finished, not consistent dates are : {not_consistent}')\n",
    "        # break\n",
    "        \n",
    "# check_all_date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Daily Log Returns\n",
    "\n",
    "The next step is to calculate the daily log returns for each stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_return_stock(code):\n",
    "    stock_data = pd.read_csv(f'./daily_data/{code}.csv', index_col=0)\n",
    "    log_return = np.log(stock_data['close']) - np.log(stock_data['close'].shift(1))\n",
    "    return stock_data.index, (log_return * 100).tolist() # percentage\n",
    "\n",
    "def cal_return_all():\n",
    "    hs300 = pd.read_csv('hs300_stocks.csv', index_col=0)\n",
    "    return_columns = []\n",
    "    \n",
    "    for i, code in enumerate(hs300['code']):\n",
    "        date_list, return_list = cal_return_stock(code)\n",
    "        return_column = pd.DataFrame(return_list, index=date_list, columns=[code])\n",
    "        return_columns.append(return_column)\n",
    "\n",
    "    return_table = pd.concat(return_columns, axis=1)\n",
    "    return_table = return_table.sort_index()\n",
    "    return_table.fillna(0, inplace=True)\n",
    "    return_table.to_csv('return_table.csv')\n",
    "\n",
    "# cal_return_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MPS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
