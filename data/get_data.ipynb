{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import baostock as bs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to calculate the realized volatility of the constituent stocks of the HS300 index. The process involves several steps, starting with fetching the constituent stocks using the Baostock API.\n",
    "\n",
    "We would like to thank [Baostock](http://baostock.com/baostock/index.php/%E9%A6%96%E9%A1%B5) for providing the data used in this research. And to facilitate the readers, we have organized the code into functions. You can simply call these functions to execute the respective tasks.\n",
    "\n",
    "### Get constituen stocks of HS300 index\n",
    "We obtain the constituent stocks of the HS300 index by using the Baostock API at the date of acquisition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "login success!\n",
      "logout success!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>updateDate</th>\n",
       "      <th>code</th>\n",
       "      <th>code_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>sh.600000</td>\n",
       "      <td>浦发银行</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>sh.600009</td>\n",
       "      <td>上海机场</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>sh.600010</td>\n",
       "      <td>包钢股份</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>sh.600011</td>\n",
       "      <td>华能国际</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>sh.600015</td>\n",
       "      <td>华夏银行</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>sz.300919</td>\n",
       "      <td>中伟股份</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>sz.300957</td>\n",
       "      <td>贝泰妮</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>sz.300979</td>\n",
       "      <td>华利集团</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>sz.300999</td>\n",
       "      <td>金龙鱼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>sz.301269</td>\n",
       "      <td>华大九天</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     updateDate       code code_name\n",
       "0    2024-01-01  sh.600000      浦发银行\n",
       "1    2024-01-01  sh.600009      上海机场\n",
       "2    2024-01-01  sh.600010      包钢股份\n",
       "3    2024-01-01  sh.600011      华能国际\n",
       "4    2024-01-01  sh.600015      华夏银行\n",
       "..          ...        ...       ...\n",
       "295  2024-01-01  sz.300919      中伟股份\n",
       "296  2024-01-01  sz.300957       贝泰妮\n",
       "297  2024-01-01  sz.300979      华利集团\n",
       "298  2024-01-01  sz.300999       金龙鱼\n",
       "299  2024-01-01  sz.301269      华大九天\n",
       "\n",
       "[300 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_constituen_stocks():\n",
    "    bs.login()\n",
    "\n",
    "    rs = bs.query_hs300_stocks(date='2024-1-6')\n",
    "\n",
    "    # get hs300 constituent stocks\n",
    "    hs300_stocks = []\n",
    "    while (rs.error_code == '0') & rs.next():\n",
    "        hs300_stocks.append(rs.get_row_data())\n",
    "    hs300 = pd.DataFrame(hs300_stocks, columns=rs.fields)\n",
    "    \n",
    "    bs.logout()\n",
    "    \n",
    "    hs300.to_csv('hs300_stocks.csv', index=True)\n",
    "    return hs300\n",
    "\n",
    "get_constituen_stocks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Stock Data Information\n",
    "\n",
    "After obtaining the constituent stocks of the HS300 index, we complete the dataset by adding additional information such as:\n",
    "\n",
    "- Abbreviated Symbol\n",
    "- Sector Code\n",
    "- Sector Code Name\n",
    "- Industry Code\n",
    "- Industry Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock info not found for sh.601059/601059.XSHG\n"
     ]
    }
   ],
   "source": [
    "def code_order_id(code):\n",
    "    if code[:3] == 'sh.':\n",
    "        return code[3:] + '.XSHG'\n",
    "    elif code[:3] == 'sz.':\n",
    "        return code[3:] + '.XSHE'\n",
    "    else:\n",
    "        Warning('code is not valid')\n",
    "        return None\n",
    "\n",
    "# add abbrev_symbol, sector_code, sector_code_name, industry_code, industry_name\n",
    "def add_extra_info():\n",
    "    hs300_stocks = pd.read_csv('hs300_stocks.csv', index_col=0)\n",
    "    all_stocks = pd.read_csv('overall_description.csv')\n",
    "\n",
    "    for code in hs300_stocks['code']:\n",
    "        order_id = code_order_id(code)\n",
    "        stock_info = all_stocks[all_stocks['order_book_id'] == order_id]\n",
    "        if len(stock_info) == 0:\n",
    "            print(f'stock info not found for {code}/{order_id}')\n",
    "            continue\n",
    "        hs300_stocks.loc[hs300_stocks['code'] == code, 'abbrev_symbol'] = stock_info['abbrev_symbol'].item()\n",
    "        hs300_stocks.loc[hs300_stocks['code'] == code, 'sector_code'] = stock_info['sector_code'].item()\n",
    "        hs300_stocks.loc[hs300_stocks['code'] == code, 'sector_code_name'] = stock_info['sector_code_name'].item()\n",
    "        hs300_stocks.loc[hs300_stocks['code'] == code, 'industry_code'] = stock_info['industry_code'].item()\n",
    "        hs300_stocks.loc[hs300_stocks['code'] == code, 'industry_name'] = stock_info['industry_name'].item()\n",
    "        \n",
    "    hs300_stocks.to_csv('hs300_stocks.csv', index=True)\n",
    "\n",
    "def group_by_industry():\n",
    "    hs300_stocks = pd.read_csv('hs300_stocks.csv', index_col=0)\n",
    "    grouped = hs300_stocks.sort_values('sector_code').reset_index()\n",
    "    grouped = grouped.rename(columns={'index' : 'original_index'})\n",
    "    grouped.to_csv('hs300_stocks.csv', index=True)\n",
    "\n",
    "add_extra_info()\n",
    "group_by_industry()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stock with code sh.601059/601059.XSHG is missing from the database and will be excluded from subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquire High-Frequency Data\n",
    "\n",
    "To analyze the high-frequency trading patterns of each stock in the HS300 index, we first acquire 5-minute interval data. This data is saved in the `/5mins_data/code.csv` format for each stock.\n",
    "\n",
    "1. **Fetching Data**: Use the Baostock API to fetch the 5-minute interval data.\n",
    "2. **Saving Data**: Save the data in the specified directory.\n",
    "3. **Data Cleaning and Processing**: Perform necessary data cleaning and processing to ensure the data is ready for analysis.\n",
    "\n",
    "*Note: To obtain daily historical data instead of 5-minute intervals, simply change the `frequency` parameter to `'d'` in the `bs.query_history_k_data_plus` function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "login success!\n",
      "get data for sz.000100 (1/300)\n",
      "get data for sz.000651 (2/300)\n",
      "get data for sh.600104 (3/300)\n",
      "get data for sh.603486 (4/300)\n",
      "get data for sz.000800 (5/300)\n",
      "get data for sh.601238 (6/300)\n",
      "get data for sh.605117 (7/300)\n",
      "get data for sh.603195 (8/300)\n",
      "get data for sz.000333 (9/300)\n",
      "get data for sh.601888 (10/300)\n",
      "get data for sh.601689 (11/300)\n",
      "get data for sz.000625 (12/300)\n",
      "get data for sh.600660 (13/300)\n",
      "get data for sh.600690 (14/300)\n",
      "get data for sh.600741 (15/300)\n",
      "get data for sh.601633 (16/300)\n",
      "get data for sz.002920 (17/300)\n",
      "get data for sh.600754 (18/300)\n",
      "get data for sh.600760 (19/300)\n",
      "get data for sz.300413 (20/300)\n",
      "get data for sz.000069 (21/300)\n",
      "get data for sh.601799 (22/300)\n",
      "get data for sz.300979 (23/300)\n",
      "get data for sz.002594 (24/300)\n",
      "get data for sh.603833 (25/300)\n",
      "get data for sh.600519 (26/300)\n",
      "get data for sh.603288 (27/300)\n",
      "get data for sh.600600 (28/300)\n",
      "get data for sh.600809 (29/300)\n",
      "get data for sh.603369 (30/300)\n",
      "get data for sz.300999 (31/300)\n",
      "get data for sh.605499 (32/300)\n",
      "get data for sh.600887 (33/300)\n",
      "get data for sz.002714 (34/300)\n",
      "get data for sz.300498 (35/300)\n",
      "get data for sz.002311 (36/300)\n",
      "get data for sz.000876 (37/300)\n",
      "get data for sz.000858 (38/300)\n",
      "get data for sz.000895 (39/300)\n",
      "get data for sh.600132 (40/300)\n",
      "get data for sz.000596 (41/300)\n",
      "get data for sz.000568 (42/300)\n",
      "get data for sz.300957 (43/300)\n",
      "get data for sz.002304 (44/300)\n",
      "get data for sh.600803 (45/300)\n",
      "get data for sh.601808 (46/300)\n",
      "get data for sh.601225 (47/300)\n",
      "get data for sh.601699 (48/300)\n",
      "get data for sh.600028 (49/300)\n",
      "get data for sh.601857 (50/300)\n",
      "get data for sh.600188 (51/300)\n",
      "get data for sh.601088 (52/300)\n",
      "get data for sz.000983 (53/300)\n",
      "get data for sh.600938 (54/300)\n",
      "get data for sh.600989 (55/300)\n",
      "get data for sh.601898 (56/300)\n",
      "get data for sh.601377 (57/300)\n",
      "get data for sh.601628 (58/300)\n",
      "get data for sh.601336 (59/300)\n",
      "get data for sh.601229 (60/300)\n",
      "get data for sh.601288 (61/300)\n",
      "get data for sh.601211 (62/300)\n",
      "get data for sh.601328 (63/300)\n",
      "get data for sh.601319 (64/300)\n",
      "get data for sh.601601 (65/300)\n",
      "get data for sh.601236 (66/300)\n",
      "get data for sh.601658 (67/300)\n",
      "get data for sh.601318 (68/300)\n",
      "get data for sh.601398 (69/300)\n",
      "get data for sh.600000 (70/300)\n",
      "get data for sh.601788 (71/300)\n",
      "get data for sz.300059 (72/300)\n",
      "get data for sz.300033 (73/300)\n",
      "get data for sz.002736 (74/300)\n",
      "get data for sz.002142 (75/300)\n",
      "get data for sz.000776 (76/300)\n",
      "get data for sz.000617 (77/300)\n",
      "get data for sz.000166 (78/300)\n",
      "get data for sz.000001 (79/300)\n",
      "get data for sh.601688 (80/300)\n",
      "get data for sh.601995 (81/300)\n",
      "get data for sh.601939 (82/300)\n",
      "get data for sh.601169 (83/300)\n",
      "get data for sh.601916 (84/300)\n",
      "get data for sh.601901 (85/300)\n",
      "get data for sh.601881 (86/300)\n",
      "get data for sh.601878 (87/300)\n",
      "get data for sh.601838 (88/300)\n",
      "get data for sh.601818 (89/300)\n",
      "get data for sh.601988 (90/300)\n",
      "get data for sh.601166 (91/300)\n",
      "get data for sh.601998 (92/300)\n",
      "get data for sh.600016 (93/300)\n",
      "get data for sh.600030 (94/300)\n",
      "get data for sh.600015 (95/300)\n",
      "get data for sh.600837 (96/300)\n",
      "get data for sh.600036 (97/300)\n",
      "get data for sh.600919 (98/300)\n",
      "get data for sh.600926 (99/300)\n",
      "get data for sh.600958 (100/300)\n",
      "get data for sh.600999 (101/300)\n",
      "get data for sh.600918 (102/300)\n",
      "get data for sh.601009 (103/300)\n",
      "get data for sh.600061 (104/300)\n",
      "get data for sh.601066 (105/300)\n",
      "get data for sh.600085 (106/300)\n",
      "get data for sz.002252 (107/300)\n",
      "get data for sz.002603 (108/300)\n",
      "get data for sh.688271 (109/300)\n",
      "get data for sh.688363 (110/300)\n",
      "get data for sz.000661 (111/300)\n",
      "get data for sz.300347 (112/300)\n",
      "get data for sz.002821 (113/300)\n",
      "get data for sz.000963 (114/300)\n",
      "get data for sz.300122 (115/300)\n",
      "get data for sz.300759 (116/300)\n",
      "get data for sz.300142 (117/300)\n",
      "get data for sh.600276 (118/300)\n",
      "get data for sz.000999 (119/300)\n",
      "get data for sh.603259 (120/300)\n",
      "get data for sz.000538 (121/300)\n",
      "get data for sz.300896 (122/300)\n",
      "get data for sz.300015 (123/300)\n",
      "get data for sh.600436 (124/300)\n",
      "get data for sh.603392 (125/300)\n",
      "get data for sh.600332 (126/300)\n",
      "get data for sz.002001 (127/300)\n",
      "get data for sh.600196 (128/300)\n",
      "get data for sz.002007 (129/300)\n",
      "get data for sz.300760 (130/300)\n",
      "get data for sh.601607 (131/300)\n",
      "get data for sh.603899 (132/300)\n",
      "get data for sh.600372 (133/300)\n",
      "get data for sh.600029 (134/300)\n",
      "get data for sh.600406 (135/300)\n",
      "get data for sz.000338 (136/300)\n",
      "get data for sh.600031 (137/300)\n",
      "get data for sh.688187 (138/300)\n",
      "get data for sh.688223 (139/300)\n",
      "get data for sz.002202 (140/300)\n",
      "get data for sz.002179 (141/300)\n",
      "get data for sz.002074 (142/300)\n",
      "get data for sh.600233 (143/300)\n",
      "get data for sz.000157 (144/300)\n",
      "get data for sz.000425 (145/300)\n",
      "get data for sh.600039 (146/300)\n",
      "get data for sh.600150 (147/300)\n",
      "get data for sh.600115 (148/300)\n",
      "get data for sz.002352 (149/300)\n",
      "get data for sz.002050 (150/300)\n",
      "get data for sh.600018 (151/300)\n",
      "get data for sh.601989 (152/300)\n",
      "get data for sh.601800 (153/300)\n",
      "get data for sz.300750 (154/300)\n",
      "get data for sh.601766 (155/300)\n",
      "get data for sh.600875 (156/300)\n",
      "get data for sh.601698 (157/300)\n",
      "get data for sh.600893 (158/300)\n",
      "get data for sh.601669 (159/300)\n",
      "get data for sh.601668 (160/300)\n",
      "get data for sz.300763 (161/300)\n",
      "get data for sh.601618 (162/300)\n",
      "get data for sh.601615 (163/300)\n",
      "get data for sh.601390 (164/300)\n",
      "get data for sh.601006 (165/300)\n",
      "get data for sh.600009 (166/300)\n",
      "get data for sh.601021 (167/300)\n",
      "get data for sh.601100 (168/300)\n",
      "get data for sh.601111 (169/300)\n",
      "get data for sh.601117 (170/300)\n",
      "get data for sh.601186 (171/300)\n",
      "get data for sz.000768 (172/300)\n",
      "get data for sh.601816 (173/300)\n",
      "get data for sh.600089 (174/300)\n",
      "get data for sz.300274 (175/300)\n",
      "get data for sz.300124 (176/300)\n",
      "get data for sh.601877 (177/300)\n",
      "get data for sz.300450 (178/300)\n",
      "get data for sh.601868 (179/300)\n",
      "get data for sh.601919 (180/300)\n",
      "get data for sz.300308 (181/300)\n",
      "get data for sh.601872 (182/300)\n",
      "get data for sz.300782 (183/300)\n",
      "get data for sz.002916 (184/300)\n",
      "get data for sz.300919 (185/300)\n",
      "get data for sz.300496 (186/300)\n",
      "get data for sz.002841 (187/300)\n",
      "get data for sz.002938 (188/300)\n",
      "get data for sz.300014 (189/300)\n",
      "get data for sz.000938 (190/300)\n",
      "get data for sz.300223 (191/300)\n",
      "get data for sz.000977 (192/300)\n",
      "get data for sz.300316 (193/300)\n",
      "get data for sz.300433 (194/300)\n",
      "get data for sz.002049 (195/300)\n",
      "get data for sz.300454 (196/300)\n",
      "get data for sz.002475 (197/300)\n",
      "get data for sz.002459 (198/300)\n",
      "get data for sz.002415 (199/300)\n",
      "get data for sz.002129 (200/300)\n",
      "get data for sz.300751 (201/300)\n",
      "get data for sz.002180 (202/300)\n",
      "get data for sz.002410 (203/300)\n",
      "get data for sz.002371 (204/300)\n",
      "get data for sz.300661 (205/300)\n",
      "get data for sz.002236 (206/300)\n",
      "get data for sz.300628 (207/300)\n",
      "get data for sz.002241 (208/300)\n",
      "get data for sz.300408 (209/300)\n",
      "get data for sz.002230 (210/300)\n",
      "get data for sh.601138 (211/300)\n",
      "get data for sz.301269 (212/300)\n",
      "get data for sh.688561 (213/300)\n",
      "get data for sh.688396 (214/300)\n",
      "get data for sh.688303 (215/300)\n",
      "get data for sh.688256 (216/300)\n",
      "get data for sh.688126 (217/300)\n",
      "get data for sh.688111 (218/300)\n",
      "get data for sh.600438 (219/300)\n",
      "get data for sh.603019 (220/300)\n",
      "get data for sh.688041 (221/300)\n",
      "get data for sh.688036 (222/300)\n",
      "get data for sh.688012 (223/300)\n",
      "get data for sh.688008 (224/300)\n",
      "get data for sh.603290 (225/300)\n",
      "get data for sh.603986 (226/300)\n",
      "get data for sh.603501 (227/300)\n",
      "get data for sh.688981 (228/300)\n",
      "get data for sh.600460 (229/300)\n",
      "get data for sh.688599 (230/300)\n",
      "get data for sz.000063 (231/300)\n",
      "get data for sh.600570 (232/300)\n",
      "get data for sz.000733 (233/300)\n",
      "get data for sh.600584 (234/300)\n",
      "get data for sz.000725 (235/300)\n",
      "get data for sh.600588 (236/300)\n",
      "get data for sh.600183 (237/300)\n",
      "get data for sh.600745 (238/300)\n",
      "get data for sh.600845 (239/300)\n",
      "get data for sh.601012 (240/300)\n",
      "get data for sh.601360 (241/300)\n",
      "get data for sh.603799 (242/300)\n",
      "get data for sh.603659 (243/300)\n",
      "get data for sh.600019 (244/300)\n",
      "get data for sh.600426 (245/300)\n",
      "get data for sh.601600 (246/300)\n",
      "get data for sh.600010 (247/300)\n",
      "get data for sh.600489 (248/300)\n",
      "get data for sh.601865 (249/300)\n",
      "get data for sh.600585 (250/300)\n",
      "get data for sh.601899 (251/300)\n",
      "get data for sz.002812 (252/300)\n",
      "get data for sh.600547 (253/300)\n",
      "get data for sh.603260 (254/300)\n",
      "get data for sz.002709 (255/300)\n",
      "get data for sz.000792 (256/300)\n",
      "get data for sz.000786 (257/300)\n",
      "get data for sz.000877 (258/300)\n",
      "get data for sz.000708 (259/300)\n",
      "get data for sh.600111 (260/300)\n",
      "get data for sh.600176 (261/300)\n",
      "get data for sz.000408 (262/300)\n",
      "get data for sz.002648 (263/300)\n",
      "get data for sh.600219 (264/300)\n",
      "get data for sh.600309 (265/300)\n",
      "get data for sz.002271 (266/300)\n",
      "get data for sh.688065 (267/300)\n",
      "get data for sh.603806 (268/300)\n",
      "get data for sh.603993 (269/300)\n",
      "get data for sz.002460 (270/300)\n",
      "get data for sz.002601 (271/300)\n",
      "get data for sh.600362 (272/300)\n",
      "get data for sh.600346 (273/300)\n",
      "get data for sz.002466 (274/300)\n",
      "get data for sz.002493 (275/300)\n",
      "get data for sh.600048 (276/300)\n",
      "get data for sh.600606 (277/300)\n",
      "get data for sz.001979 (278/300)\n",
      "get data for sh.600732 (279/300)\n",
      "get data for sh.601155 (280/300)\n",
      "get data for sz.000002 (281/300)\n",
      "get data for sh.600515 (282/300)\n",
      "get data for sh.600050 (283/300)\n",
      "get data for sh.600941 (284/300)\n",
      "get data for sh.601728 (285/300)\n",
      "get data for sz.002555 (286/300)\n",
      "get data for sz.002027 (287/300)\n",
      "get data for sh.601985 (288/300)\n",
      "get data for sh.600795 (289/300)\n",
      "get data for sh.600674 (290/300)\n",
      "get data for sz.000301 (291/300)\n",
      "get data for sh.600886 (292/300)\n",
      "get data for sh.600900 (293/300)\n",
      "get data for sh.600905 (294/300)\n",
      "get data for sh.600023 (295/300)\n",
      "get data for sz.003816 (296/300)\n",
      "get data for sh.600025 (297/300)\n",
      "get data for sh.600011 (298/300)\n",
      "get data for sz.001289 (299/300)\n",
      "get data for sh.601059 (300/300)\n",
      "logout success!\n",
      "data download completed!\n",
      "date information added to hs300_stocks.csv\n"
     ]
    }
   ],
   "source": [
    "def get_stock_data(code):\n",
    "    start_time = time.time()\n",
    "    rs = bs.query_history_k_data_plus(code,\n",
    "        \"date, time, code, open, high, low, close, volume\",\n",
    "        start_date='2006-01-01', end_date='2023-12-31',\n",
    "        frequency=\"5\", adjustflag=\"1\")\n",
    "    \n",
    "    # if there is an error when downloading data, return False\n",
    "    if rs.error_code != '0':\n",
    "        return False\n",
    "    # print(f'recived data for {code}, time: {(time.time() - start_time):.1f}')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    data_list = []\n",
    "    while (rs.error_code == '0') & rs.next():\n",
    "        new_row = rs.get_row_data()\n",
    "        trade_time = pd.to_datetime(new_row[1], format='%Y%m%d%H%M%S%f')\n",
    "        new_row[1] = trade_time.strftime('%H:%M')\n",
    "        data_list.append(new_row)\n",
    "    result = pd.DataFrame(data_list, columns=rs.fields)\n",
    "    result.to_csv(f'./5mins_data/{code}.csv', index=False)\n",
    "    # print(f'convert data to dataframe, time: {(time.time() - start_time):.1f}')\n",
    "    \n",
    "    return True\n",
    "\n",
    "def get_data():\n",
    "    hs300 = pd.read_csv('hs300_stocks.csv', index_col=0)\n",
    "    bs.login()\n",
    "\n",
    "    code_list = hs300['code']\n",
    "    for i, code in enumerate(code_list):\n",
    "        print(f'get data for {code} ({i + 1}/300)')\n",
    "        if os.path.exists(f'./5mins_data/{code}.csv'):\n",
    "            print(f'{code} already exists, skip')\n",
    "            continue\n",
    "        while get_stock_data(code) == False:\n",
    "            print(f'error occured when downloading data for {code}, retrying...')\n",
    "            print('if this error persists, please check the error code in baostock (by printing rs.error_msg) or check your network connection.')\n",
    "            time.sleep(5)\n",
    "\n",
    "    bs.logout()\n",
    "    \n",
    "    print('data download completed!')\n",
    "\n",
    "get_data()\n",
    "\n",
    "# add back start_date and end_date of trading to hs300_stocks.csv\n",
    "def add_date_info():\n",
    "    hs300 = pd.read_csv('hs300_stocks.csv', index_col=0)\n",
    "    for code in hs300['code']:\n",
    "        stock_data = pd.read_csv(f'./5mins_data/{code}.csv')\n",
    "        start_date = stock_data['date'].iloc[0]\n",
    "        end_date = stock_data['date'].iloc[-1]\n",
    "        hs300.loc[hs300['code'] == code, 'start_date'] = start_date\n",
    "        hs300.loc[hs300['code'] == code, 'end_date'] = end_date\n",
    "    hs300.to_csv('hs300_stocks.csv')\n",
    "    \n",
    "    print('date information added to hs300_stocks.csv')\n",
    "\n",
    "add_date_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RV Calculation\n",
    "We calculate the realized volatility for each stock, and then combine it into a single dataframe (/realized_volatility/rv_table.csv) with index to be unique date and columns to be stock code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/300 stocks calculated, elapsed time: 780.3\n",
      "100/300 stocks calculated, elapsed time: 718.4\n",
      "150/300 stocks calculated, elapsed time: 784.0\n",
      "200/300 stocks calculated, elapsed time: 629.2\n",
      "250/300 stocks calculated, elapsed time: 579.1\n",
      "300/300 stocks calculated, elapsed time: 744.3\n"
     ]
    }
   ],
   "source": [
    "# method to calculate the realized volatility of each day\n",
    "def cal_rv_stock(code):\n",
    "    stock_data = pd.read_csv(f'./5mins_data/{code}.csv')\n",
    "    stock_data['log_return'] = np.log(stock_data['close']) - np.log(stock_data['close'].shift(1))\n",
    "    date_list = stock_data['date'].unique()\n",
    "    rv_list = []\n",
    "    filter_time = stock_data[stock_data['time'] >= '10:00']\n",
    "    for date in date_list:\n",
    "        filter1 = filter_time['date'] == date\n",
    "        log_returns = filter_time[filter1]['log_return'] * 100 # percentage\n",
    "        if len(log_returns) != 43:\n",
    "            Warning(f'code: {code}, date: {date}, there may exist missing data')\n",
    "        rv = np.sum(log_returns ** 2)\n",
    "        rv_list.append(rv)\n",
    "    \n",
    "    return date_list, rv_list\n",
    "\n",
    "def cal_rv_all():\n",
    "    hs300 = pd.read_csv('hs300_stocks.csv', index_col=0)\n",
    "    rv_columns = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, code in enumerate(hs300['code']):\n",
    "        date_list, rv_list = cal_rv_stock(code)\n",
    "        rv_column = pd.DataFrame(rv_list, index=date_list, columns=[code])\n",
    "        rv_columns.append(rv_column)\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f'{i + 1}/300 stocks calculated, elapsed time: {(time.time() - start_time):.1f}')\n",
    "            start_time = time.time()\n",
    "\n",
    "    rv_table = pd.concat(rv_columns, axis=1)\n",
    "    rv_table = rv_table.sort_index()\n",
    "    rv_table.to_csv('realized_volatility/rv_table.csv')\n",
    "\n",
    "cal_rv_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Check the Data\n",
    "\n",
    "It's essential to verify the integrity of the data in the `rv_table`. Specifically, we need to ensure that any `NaN` values in the `rv_table` are due to non-trading days. This step helps in maintaining the accuracy of our analysis and ensures that the missing data points are not due to errors in data acquisition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "login success!\n",
      "code: sz.002920, na but trading: ['2022-02-21']\n",
      "code: sh.600809, non trading but not na: ['2006-04-20']\n",
      "code: sh.600436, non trading but not na: ['2006-04-20']\n",
      "code: sh.600372, non trading but not na: ['2007-04-20', '2007-04-24', '2007-04-27']\n",
      "code: sh.600089, non trading but not na: ['2006-04-20']\n",
      "code: sh.600570, na but trading: ['2018-08-16']\n",
      "code: sh.601360, na but trading: ['2018-02-12', '2018-02-28']\n",
      "code: sh.600309, non trading but not na: ['2006-04-20']\n",
      "logout success!\n"
     ]
    }
   ],
   "source": [
    "# get the trading status stocks of each day\n",
    "# save the daily price data to csv file\n",
    "def get_tradestatus(code, start_date, end_date):\n",
    "    if os.path.exists(f'./daily_data/{code}.csv'):\n",
    "        return True\n",
    "    rs = bs.query_history_k_data_plus(code,\n",
    "        \"date, code, tradestatus, open, high, low, close, volume\",\n",
    "        start_date=start_date, end_date=end_date,\n",
    "        frequency=\"d\", adjustflag=\"1\")\n",
    "    if rs.error_code != '0':\n",
    "        return False\n",
    "    \n",
    "    data_list = []\n",
    "    while (rs.error_code == '0') & rs.next():\n",
    "        data_list.append(rs.get_row_data())\n",
    "    result = pd.DataFrame(data_list, columns=rs.fields)\n",
    "    result.to_csv(f'./daily_data/{code}.csv', index=False)\n",
    "\n",
    "    return True\n",
    "\n",
    "# check the consistency of the trading status and the realized volatility\n",
    "def double_check(code, na_dates_in_rv_table):\n",
    "    not_consistent = []\n",
    "    daily_data = pd.read_csv(f'./daily_data/{code}.csv', index_col=0)\n",
    "    non_trading_dates = daily_data[daily_data['tradestatus'] == 0].index\n",
    "    na_dates_in_rv_table = na_dates_in_rv_table[(na_dates_in_rv_table >= daily_data.index[0]) & (na_dates_in_rv_table <= daily_data.index[-1])]\n",
    "    \n",
    "    na_but_trading = [date for date in na_dates_in_rv_table if date not in non_trading_dates]\n",
    "    na_but_trading_copy = na_but_trading.copy()\n",
    "    non_trading_but_not_na = [date for date in non_trading_dates if date not in na_dates_in_rv_table]\n",
    "    for date in na_but_trading_copy:\n",
    "        if date not in daily_data.index:\n",
    "            na_but_trading.remove(date)\n",
    "    \n",
    "    if len(na_but_trading) != 0:\n",
    "        print(f'code: {code}, na but trading: {na_but_trading}')\n",
    "    if len(non_trading_but_not_na) != 0:\n",
    "        print(f'code: {code}, non trading but not na: {non_trading_but_not_na}')\n",
    "    \n",
    "    not_consistent = na_but_trading + non_trading_but_not_na\n",
    "    \n",
    "    return not_consistent\n",
    "\n",
    "def check_all_date():\n",
    "    rv_table = pd.read_csv('realized_volatility/rv_table.csv', index_col=0)\n",
    "    hs300_stocks = pd.read_csv('hs300_stocks.csv', index_col=0)\n",
    "    code_list = rv_table.columns\n",
    "\n",
    "    bs.login()\n",
    "    \n",
    "    for code in code_list:\n",
    "        start_date = hs300_stocks[hs300_stocks['code'] == code]['start_date'].iloc[0]\n",
    "        end_date = hs300_stocks[hs300_stocks['code'] == code]['end_date'].iloc[0]\n",
    "        na_dates = rv_table[rv_table[code].isna()].index\n",
    "        na_dates = na_dates[(na_dates >= start_date) & (na_dates <= end_date)]\n",
    "        \n",
    "        while get_tradestatus(code, start_date, end_date) == False:\n",
    "            print(f'error occured when downloading daily data for {code}, retrying...')\n",
    "            time.sleep(5)\n",
    "        \n",
    "        not_consistent = double_check(code, na_dates)\n",
    "    \n",
    "    bs.logout()\n",
    "        \n",
    "check_all_date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows that most of the data is consistent. However, three stocks have inconsistencies (trading days without downloaded records) within the sample period (2014-01-01 to 2023-12-31). We will resolve these issues during the data cleaning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Daily Log Returns\n",
    "\n",
    "The next step is to calculate the daily log returns for each stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return table saved\n"
     ]
    }
   ],
   "source": [
    "def cal_return_stock(code):\n",
    "    stock_data = pd.read_csv(f'./daily_data/{code}.csv', index_col=0)\n",
    "    log_return = np.log(stock_data['close']) - np.log(stock_data['close'].shift(1))\n",
    "    return stock_data.index, (log_return * 100).tolist() # percentage\n",
    "\n",
    "def cal_return_all():\n",
    "    hs300 = pd.read_csv('hs300_stocks.csv', index_col=0)\n",
    "    return_columns = []\n",
    "    \n",
    "    for i, code in enumerate(hs300['code']):\n",
    "        date_list, return_list = cal_return_stock(code)\n",
    "        return_column = pd.DataFrame(return_list, index=date_list, columns=[code])\n",
    "        return_columns.append(return_column)\n",
    "\n",
    "    return_table = pd.concat(return_columns, axis=1)\n",
    "    return_table = return_table.sort_index()\n",
    "    return_table.fillna(0, inplace=True)\n",
    "    return_table.to_csv('return_table.csv')\n",
    "    \n",
    "    print('return table saved')\n",
    "\n",
    "cal_return_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MPS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
